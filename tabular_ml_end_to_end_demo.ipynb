{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198aa981",
   "metadata": {},
   "source": [
    "\n",
    "# Tabular ML End-to-End Demo\n",
    "\n",
    "This notebook mirrors the stage-wise experience of the `tabular_ml` UI while keeping everything offline, reproducible, and keyboard-friendly. It walks you through a complete pipeline for both classification and regression tasks on tabular data.\n",
    "\n",
    "## Table of Contents\n",
    "- [1. Environment Check & Imports](#environment-check)\n",
    "- [2. Helper Utilities & Global State](#helper-utilities)\n",
    "- [3. Configuration](#configuration)\n",
    "- [4. Data Loading](#data-loading)\n",
    "- [5. Target Detection & Basic Cleaning](#target-detection)\n",
    "- [6. Exploratory Data Analysis (EDA)](#eda)\n",
    "- [7. Outlier Detection & Removal](#outliers)\n",
    "- [8. Preprocessing Pipeline](#preprocessing)\n",
    "- [9. Data Splitting Strategy](#splitting)\n",
    "- [10. Model Zoo Setup](#model-zoo)\n",
    "- [11. Training & Live Progress](#training)\n",
    "- [12. Evaluation](#evaluation)\n",
    "- [13. Inference Demo](#inference)\n",
    "- [14. Run Summary](#run-summary)\n",
    "- [15. Optional Export Cells](#optional-export)\n",
    "- [16. Testing Checklist](#testing-checklist)\n",
    "\n",
    "**Pipeline stages at a glance**\n",
    "- Data ingestion → exploration → outlier handling → preprocessing → split → modeling → training → evaluation → inference.\n",
    "- Designed for rapid iteration with sensible defaults and reproducible results.\n",
    "\n",
    "> **What you'll need**  \n",
    "> Required: Python ≥3.9, pandas, numpy, scikit-learn, matplotlib, scipy.  \n",
    "> Optional (auto-detected): xgboost, lightgbm, torch.  \n",
    "> Run the next cell to see which versions are available in your environment.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Quick Demo Path vs Full Walkthrough</strong></summary>\n",
    "\n",
    "- **Quick Demo:** Keep defaults, use the built-in Titanic-like dataset, and step through the cells sequentially.\n",
    "- **Full Walkthrough:** Adjust the configuration cell, experiment with custom datasets, and explore optional models/exports.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104caa1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment Check & Imports <a id=\"environment-check\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Verifies required/optional dependencies and records their versions.\n",
    "- Configures matplotlib for notebook-friendly visuals and sets global seeds.\n",
    "- Establishes a capability matrix so later sections can adapt automatically.\n",
    "\n",
    "➡️ **Run this cell next.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7edbea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import importlib\n",
    "import math\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "OPTIONAL_MODULES: Dict[str, Any] = {}\n",
    "OPTIONAL_SPECS = {\n",
    "    \"xgboost\": \"xgboost\",\n",
    "    \"lightgbm\": \"lightgbm\",\n",
    "    \"torch\": \"torch\",\n",
    "}\n",
    "for friendly_name, import_path in OPTIONAL_SPECS.items():\n",
    "    spec = importlib.util.find_spec(import_path)\n",
    "    if spec is not None:\n",
    "        module = importlib.import_module(import_path)\n",
    "        OPTIONAL_MODULES[friendly_name] = module\n",
    "\n",
    "HAS_XGBOOST = \"xgboost\" in OPTIONAL_MODULES\n",
    "HAS_LIGHTGBM = \"lightgbm\" in OPTIONAL_MODULES\n",
    "HAS_TORCH = \"torch\" in OPTIONAL_MODULES\n",
    "try:\n",
    "    import scipy\n",
    "    from scipy import stats as scipy_stats  # type: ignore\n",
    "    HAS_SCIPY = True\n",
    "except Exception:  # noqa: BLE001\n",
    "    scipy = None\n",
    "    scipy_stats = None\n",
    "    HAS_SCIPY = False\n",
    "\n",
    "version_rows = [\n",
    "    (\"python\", platform.python_version()),\n",
    "    (\"pandas\", pd.__version__),\n",
    "    (\"numpy\", np.__version__),\n",
    "    (\"scikit-learn\", sklearn_version),\n",
    "    (\"matplotlib\", plt.matplotlib.__version__),\n",
    "    (\"scipy\", getattr(scipy, \"__version__\", \"❌ missing\")),\n",
    "]\n",
    "version_rows.append((\"xgboost\", OPTIONAL_MODULES.get(\"xgboost\").__version__ if HAS_XGBOOST else \"❌ missing\"))\n",
    "version_rows.append((\"lightgbm\", OPTIONAL_MODULES.get(\"lightgbm\").__version__ if HAS_LIGHTGBM else \"❌ missing\"))\n",
    "version_rows.append((\"torch\", OPTIONAL_MODULES.get(\"torch\").__version__ if HAS_TORCH else \"❌ missing\"))\n",
    "\n",
    "version_df = pd.DataFrame(version_rows, columns=[\"Library\", \"Version / Status\"])\n",
    "display(Markdown(\"**Package availability overview**\"))\n",
    "display(version_df)\n",
    "\n",
    "capability_rows = [\n",
    "    (\"GPU Available (torch)\", \"✅\" if HAS_TORCH and OPTIONAL_MODULES[\"torch\"].cuda.is_available() else \"⚠️/❌\"),\n",
    "    (\"XGBoost Enabled\", \"✅\" if HAS_XGBOOST else \"❌\"),\n",
    "    (\"LightGBM Enabled\", \"✅\" if HAS_LIGHTGBM else \"❌\"),\n",
    "    (\"SciPy Stats\", \"✅\" if HAS_SCIPY else \"⚠️ (skew/kurtosis disabled)\"),\n",
    "]\n",
    "capability_df = pd.DataFrame(capability_rows, columns=[\"Capability\", \"Status\"])\n",
    "display(capability_df)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if HAS_TORCH:\n",
    "    OPTIONAL_MODULES[\"torch\"].manual_seed(RANDOM_SEED)\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\": (8, 5),\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 11,\n",
    "        \"ytick.labelsize\": 11,\n",
    "        \"legend.fontsize\": 11,\n",
    "        \"grid.alpha\": 0.3,\n",
    "        \"axes.grid\": True,\n",
    "        \"figure.autolayout\": True,\n",
    "    }\n",
    ")\n",
    "print(f\"Global random seed set to {RANDOM_SEED}. Deterministic ops depend on library guarantees.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00a473",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Helper Utilities & Global State <a id=\"helper-utilities\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Defines reusable helper functions for dataset synthesis, heuristics, plotting, and evaluation.\n",
    "- Centralises mutable state (`PIPELINE_STATE`) shared across subsequent steps.\n",
    "- Keeps implementation concise while maintaining readability via comments.\n",
    "\n",
    "➡️ **Run this cell after the imports.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d16700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    precision_recall_curve,\n",
    "    r2_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "PIPELINE_STATE: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "def synthesize_titanic_like(n_rows: int = 800, seed: int = RANDOM_SEED) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pclass = rng.integers(1, 4, size=n_rows)\n",
    "    sex = rng.choice([\"male\", \"female\"], size=n_rows)\n",
    "    embarked = rng.choice([\"S\", \"C\", \"Q\"], size=n_rows, p=[0.72, 0.18, 0.10])\n",
    "    age = np.clip(rng.normal(29.7, 13.0, size=n_rows), 0.4, 80)\n",
    "    sibsp = rng.integers(0, 5, size=n_rows)\n",
    "    parch = rng.integers(0, 4, size=n_rows)\n",
    "    fare = np.round(np.clip(rng.normal(32.0, 49.0, size=n_rows), 4, 512), 2)\n",
    "    cabin_known = rng.choice([0, 1], size=n_rows, p=[0.7, 0.3])\n",
    "    title = np.where(age < 16, \"Master\", np.where(sex == \"female\", \"Mrs\", \"Mr\"))\n",
    "    family_size = sibsp + parch + 1\n",
    "    survived_logits = (\n",
    "        -1.2 * (pclass - 1)\n",
    "        + 0.8 * (sex == \"female\").astype(float)\n",
    "        + 0.3 * cabin_known\n",
    "        + 0.02 * (family_size <= 4)\n",
    "        + rng.normal(0, 0.8, size=n_rows)\n",
    "    )\n",
    "    survived = (1 / (1 + np.exp(-survived_logits)) > 0.5).astype(int)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"PassengerId\": np.arange(1, n_rows + 1),\n",
    "            \"Survived\": survived,\n",
    "            \"Pclass\": pclass,\n",
    "            \"Name_Title\": title,\n",
    "            \"Sex\": sex,\n",
    "            \"Age\": np.round(age, 1),\n",
    "            \"SibSp\": sibsp,\n",
    "            \"Parch\": parch,\n",
    "            \"FamilySize\": family_size,\n",
    "            \"Fare\": fare,\n",
    "            \"CabinKnown\": cabin_known,\n",
    "            \"Embarked\": embarked,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    mask_age = rng.random(n_rows) < 0.1\n",
    "    mask_embarked = rng.random(n_rows) < 0.03\n",
    "    df.loc[mask_age, \"Age\"] = np.nan\n",
    "    df.loc[mask_embarked, \"Embarked\"] = np.nan\n",
    "    df.loc[df.sample(frac=0.02, random_state=seed).index, \"Fare\"] = np.nan\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_local_dataset(path_str: str, max_bytes: int = 50 * 1024 * 1024) -> pd.DataFrame:\n",
    "    path = Path(path_str).expanduser()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    if path.stat().st_size > max_bytes:\n",
    "        raise ValueError(f\"File is larger than {max_bytes / (1024 * 1024):.1f} MB; please load a smaller sample.\")\n",
    "\n",
    "    if path.suffix.lower() in {\".xls\", \".xlsx\"}:\n",
    "        return pd.read_excel(path)\n",
    "\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin-1\")\n",
    "    except pd.errors.ParserError:\n",
    "        return pd.read_csv(path, sep=\";\")\n",
    "\n",
    "\n",
    "def memory_usage_mb(df: pd.DataFrame) -> float:\n",
    "    return float(df.memory_usage(deep=True).sum() / (1024 ** 2))\n",
    "\n",
    "\n",
    "def suggest_target_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    priority_names = [\"target\", \"label\", \"survived\", \"outcome\", \"y\"]\n",
    "    lower_name_map = {col.lower(): col for col in df.columns}\n",
    "    for name in priority_names:\n",
    "        if name in lower_name_map:\n",
    "            return lower_name_map[name]\n",
    "    binary_candidates = [\n",
    "        col\n",
    "        for col in df.columns\n",
    "        if df[col].nunique(dropna=True) <= 2 and df[col].dtype != object and not col.lower().startswith(\"id\")\n",
    "    ]\n",
    "    if binary_candidates:\n",
    "        return binary_candidates[0]\n",
    "    if df.columns.size >= 2:\n",
    "        return df.columns[-1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def drop_redundant_columns(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:\n",
    "    dropped: Dict[str, List[str]] = {\"constant\": [], \"duplicates\": []}\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique(dropna=False) <= 1:\n",
    "            dropped[\"constant\"].append(col)\n",
    "    df = df.drop(columns=dropped[\"constant\"], errors=\"ignore\")\n",
    "    duplicate_mask = df.T.duplicated()\n",
    "    duplicate_cols = df.columns[duplicate_mask]\n",
    "    if len(duplicate_cols) > 0:\n",
    "        dropped[\"duplicates\"] = list(duplicate_cols)\n",
    "        df = df.loc[:, ~duplicate_mask]\n",
    "    return df, dropped\n",
    "\n",
    "\n",
    "def resolve_task_type(y: pd.Series, explicit: str = \"auto\") -> str:\n",
    "    if explicit in {\"classification\", \"regression\"}:\n",
    "        return explicit\n",
    "    inferred = type_of_target(y)\n",
    "    if inferred in {\"binary\", \"multiclass\", \"multilabel-indicator\"}:\n",
    "        return \"classification\"\n",
    "    return \"regression\"\n",
    "\n",
    "\n",
    "def describe_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    summary = df.describe().T\n",
    "    if HAS_SCIPY:\n",
    "        summary[\"skew\"] = df.apply(lambda s: scipy_stats.skew(s.dropna()) if s.dropna().size else np.nan)\n",
    "        summary[\"kurtosis\"] = df.apply(lambda s: scipy_stats.kurtosis(s.dropna()) if s.dropna().size else np.nan)\n",
    "    summary[\"missing\"] = df.isna().sum()\n",
    "    return summary\n",
    "\n",
    "\n",
    "def describe_categorical(df: pd.DataFrame, top_k: int = 5) -> Dict[str, pd.Series]:\n",
    "    summaries: Dict[str, pd.Series] = {}\n",
    "    for col in df.columns:\n",
    "        counts = df[col].astype(\"object\").value_counts(dropna=False).head(top_k)\n",
    "        summaries[col] = counts\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def plot_missingness(df: pd.DataFrame, top_n: int = 15) -> None:\n",
    "    missing = df.isna().sum().sort_values(ascending=False)\n",
    "    missing = missing[missing > 0].head(top_n)\n",
    "    if missing.empty:\n",
    "        print(\"No missing values to visualise.\")\n",
    "        return\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(missing.index, missing.values, color=\"#386cb0\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Missing count\")\n",
    "    plt.title(\"Top missing columns\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_numeric_distributions(df: pd.DataFrame, max_features: int = 4) -> None:\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        print(\"No numeric columns available for distribution plots.\")\n",
    "        return\n",
    "    for col in numeric_cols[:max_features]:\n",
    "        col_data = df[col].dropna()\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.hist(col_data, bins=30, color=\"#7fc97f\", alpha=0.7, density=False)\n",
    "        if HAS_SCIPY and col_data.size > 1:\n",
    "            kde = scipy_stats.gaussian_kde(col_data)\n",
    "            xs = np.linspace(col_data.min(), col_data.max(), 200)\n",
    "            plt.plot(xs, kde(xs) * (col_data.size * (xs[1] - xs[0])), color=\"#f0027f\", linewidth=2, label=\"KDE\")\n",
    "            plt.legend()\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.DataFrame, max_features: int = 20) -> None:\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numeric_cols) <= 1:\n",
    "        print(\"Not enough numeric columns for correlation heatmap.\")\n",
    "        return\n",
    "    variances = df[numeric_cols].var().sort_values(ascending=False)\n",
    "    top_cols = variances.head(max_features).index\n",
    "    corr = df[top_cols].corr()\n",
    "    plt.figure(figsize=(min(0.5 * len(top_cols) + 4, 12), min(0.5 * len(top_cols) + 4, 12)))\n",
    "    plt.imshow(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    plt.colorbar(label=\"Correlation\")\n",
    "    plt.xticks(range(len(top_cols)), top_cols, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(top_cols)), top_cols)\n",
    "    plt.title(\"Correlation heatmap (top variance numerics)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_target_relationships(X: pd.DataFrame, y: pd.Series, task: str, max_features: int = 3) -> None:\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if task == \"classification\":\n",
    "        target_counts = y.value_counts()\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(target_counts.index.astype(str), target_counts.values, color=\"#beaed4\")\n",
    "        plt.title(\"Target class distribution\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.show()\n",
    "        for col in numeric_cols[:max_features]:\n",
    "            plt.figure(figsize=(7, 4))\n",
    "            data = [X.loc[y == cls, col].dropna() for cls in sorted(y.unique())]\n",
    "            plt.boxplot(data, labels=[str(cls) for cls in sorted(y.unique())], showmeans=True)\n",
    "            plt.title(f\"{col} vs target\")\n",
    "            plt.xlabel(\"Target class\")\n",
    "            plt.ylabel(col)\n",
    "            plt.show()\n",
    "    else:\n",
    "        target_vals = y.values\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(target_vals, bins=30, color=\"#fdc086\", alpha=0.8)\n",
    "        plt.title(\"Target distribution (regression)\")\n",
    "        plt.xlabel(\"Target value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "        if numeric_cols:\n",
    "            correlations = (\n",
    "                X[numeric_cols]\n",
    "                .apply(lambda col: np.corrcoef(col.fillna(col.mean()), y)[0, 1])\n",
    "                .abs()\n",
    "                .sort_values(ascending=False)\n",
    "            )\n",
    "            for col in correlations.head(max_features).index:\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                plt.scatter(X[col], y, alpha=0.6, color=\"#386cb0\")\n",
    "                plt.title(f\"{col} vs target\")\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel(\"Target\")\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "def compute_outlier_mask(df: pd.DataFrame, method: str = \"IQR\", threshold: float = 1.5) -> pd.Series:\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    if numeric_df.empty:\n",
    "        return pd.Series(False, index=df.index)\n",
    "    if method == \"IQR\":\n",
    "        q1 = numeric_df.quantile(0.25)\n",
    "        q3 = numeric_df.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - threshold * iqr\n",
    "        upper = q3 + threshold * iqr\n",
    "        mask = ((numeric_df < lower) | (numeric_df > upper)).any(axis=1)\n",
    "        return mask\n",
    "    if method == \"ZScore\":\n",
    "        means = numeric_df.mean()\n",
    "        stds = numeric_df.std(ddof=0).replace(0, np.nan)\n",
    "        zscores = (numeric_df - means) / stds\n",
    "        mask = zscores.abs().gt(threshold).any(axis=1)\n",
    "        return mask.fillna(False)\n",
    "    if method == \"IsolationForest\":\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "\n",
    "        iso = IsolationForest(random_state=RANDOM_SEED, contamination=\"auto\")\n",
    "        preds = iso.fit_predict(numeric_df.fillna(numeric_df.median()))\n",
    "        return pd.Series(preds == -1, index=df.index)\n",
    "    return pd.Series(False, index=df.index)\n",
    "\n",
    "\n",
    "def build_preprocessing_pipeline(\n",
    "    X: pd.DataFrame,\n",
    "    scaling: Optional[str] = None,\n",
    "    encoding: Optional[str] = \"onehot\",\n",
    "    impute_strategy_num: str = \"median\",\n",
    "    impute_strategy_cat: str = \"most_frequent\",\n",
    ") -> ColumnTransformer:\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = [col for col in X.columns if col not in numeric_cols]\n",
    "\n",
    "    transformers: List[Tuple[str, Pipeline, List[str]]] = []\n",
    "    if numeric_cols:\n",
    "        steps: List[Tuple[str, Any]] = [(\"imputer\", SimpleImputer(strategy=impute_strategy_num))]\n",
    "        if scaling == \"standard\":\n",
    "            steps.append((\"scaler\", StandardScaler()))\n",
    "        elif scaling == \"minmax\":\n",
    "            steps.append((\"scaler\", MinMaxScaler()))\n",
    "        transformers.append((\"num\", Pipeline(steps=steps), numeric_cols))\n",
    "\n",
    "    if categorical_cols and encoding:\n",
    "        if encoding == \"onehot\":\n",
    "            encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "        else:\n",
    "            encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        transformers.append(\n",
    "            (\n",
    "                \"cat\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\"imputer\", SimpleImputer(strategy=impute_strategy_cat, fill_value=\"missing\")),\n",
    "                        (\"encoder\", encoder),\n",
    "                    ]\n",
    "                ),\n",
    "                categorical_cols,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if not transformers:\n",
    "        raise ValueError(\"No transformers configured; check your feature columns.\")\n",
    "\n",
    "    return ColumnTransformer(transformers=transformers)\n",
    "\n",
    "\n",
    "def format_seconds(seconds: float) -> str:\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    minutes, secs = divmod(seconds, 60)\n",
    "    if minutes < 60:\n",
    "        return f\"{int(minutes)}m {secs:.0f}s\"\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return f\"{int(hours)}h {int(minutes)}m\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelResult:\n",
    "    name: str\n",
    "    pipeline: Pipeline\n",
    "    train_time: float\n",
    "    val_metric: Optional[float]\n",
    "    test_metric: Optional[float]\n",
    "    metrics: Dict[str, float]\n",
    "    task_type: str\n",
    "    additional: Dict[str, Any]\n",
    "\n",
    "PIPELINE_STATE[\"helper_ready\"] = True\n",
    "print(\"Helper utilities loaded. STATE dictionary initialised as PIPELINE_STATE.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7537a8c",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Configuration <a id=\"configuration\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Collects key knobs that control dataset handling, preprocessing, and modeling.\n",
    "- Auto-populates defaults based on environment detection while allowing quick overrides.\n",
    "- Displays the resolved configuration so every run is transparent and reproducible.\n",
    "\n",
    "➡️ **Adjust values as needed, then run this cell.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814551f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not PIPELINE_STATE.get(\"helper_ready\"):\n",
    "    raise RuntimeError(\"Run the helper utilities cell before configuring the pipeline.\")\n",
    "\n",
    "config: Dict[str, Any] = {\n",
    "    \"task_type\": \"auto\",  # 'classification' | 'regression' | 'auto'\n",
    "    \"target_column\": None,\n",
    "    \"test_size\": 0.2,\n",
    "    \"val_size\": 0.1,\n",
    "    \"cv_folds\": 0,\n",
    "    \"scaling\": \"standard\",  # None | 'standard' | 'minmax'\n",
    "    \"outlier_method\": None,  # None | 'IQR' | 'ZScore' | 'IsolationForest'\n",
    "    \"outlier_threshold\": 1.5,\n",
    "    \"categorical_encoding\": \"onehot\",  # None | 'onehot' | 'ordinal'\n",
    "    \"enable_pytorch_nn\": HAS_TORCH,\n",
    "    \"enable_xgboost\": HAS_XGBOOST,\n",
    "    \"enable_lightgbm\": HAS_LIGHTGBM,\n",
    "    \"max_rows_preview\": 5,\n",
    "    \"impute_numeric\": \"median\",\n",
    "    \"impute_categorical\": \"most_frequent\",\n",
    "    \"allow_export\": False,\n",
    "}\n",
    "\n",
    "PIPELINE_STATE[\"config\"] = config\n",
    "\n",
    "display(pd.DataFrame(list(config.items()), columns=[\"Parameter\", \"Value\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58321df4",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Data Loading <a id=\"data-loading\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Offers a deterministic built-in Titanic-style dataset for offline demos.\n",
    "- Provides a hook to load local CSV/Excel files with safety checks.\n",
    "- Summarises the loaded data: shape, dtypes, memory footprint, missing values, and quick previews.\n",
    "\n",
    "➡️ **Choose the data source and run the cell.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = PIPELINE_STATE.get(\"config\", {})\n",
    "\n",
    "# --- User toggles ---\n",
    "USE_BUILTIN_DATA = True  # Set to False to load a local file.\n",
    "CUSTOM_FILE_PATH = \"\"  # Provide a path when USE_BUILTIN_DATA is False.\n",
    "PREVIEW_RANDOM_SEED = RANDOM_SEED\n",
    "\n",
    "if USE_BUILTIN_DATA:\n",
    "    df_loaded = synthesize_titanic_like(seed=RANDOM_SEED)\n",
    "    data_source = \"Built-in Titanic-like demo\"\n",
    "else:\n",
    "    if not CUSTOM_FILE_PATH:\n",
    "        raise ValueError(\"Please set CUSTOM_FILE_PATH when USE_BUILTIN_DATA is False.\")\n",
    "    df_loaded = load_local_dataset(CUSTOM_FILE_PATH)\n",
    "    data_source = f\"User file: {CUSTOM_FILE_PATH}\"\n",
    "\n",
    "PIPELINE_STATE[\"df_raw\"] = df_loaded.copy()\n",
    "PIPELINE_STATE[\"data_source\"] = data_source\n",
    "\n",
    "print(f\"Data source: {data_source}\")\n",
    "print(f\"Shape: {df_loaded.shape[0]} rows × {df_loaded.shape[1]} columns\")\n",
    "print(f\"Memory usage: {memory_usage_mb(df_loaded):.2f} MB\")\n",
    "print(\"Dtypes:\")\n",
    "display(df_loaded.dtypes.to_frame(name=\"dtype\"))\n",
    "\n",
    "missing_counts = df_loaded.isna().sum()\n",
    "if missing_counts.any():\n",
    "    display(missing_counts[missing_counts > 0].to_frame(name=\"missing_values\"))\n",
    "else:\n",
    "    print(\"No missing values detected.\")\n",
    "\n",
    "preview_rows = config.get(\"max_rows_preview\", 5)\n",
    "print(f\"Top {preview_rows} rows:\")\n",
    "display(df_loaded.head(preview_rows))\n",
    "print(f\"Bottom {preview_rows} rows:\")\n",
    "display(df_loaded.tail(preview_rows))\n",
    "print(\"Random sample:\")\n",
    "display(df_loaded.sample(min(preview_rows, len(df_loaded)), random_state=PREVIEW_RANDOM_SEED))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd38b36",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Target Detection & Basic Cleaning <a id=\"target-detection\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Suggests a target column if none is configured, based on names and cardinality.\n",
    "- Removes constant and duplicate feature columns for a cleaner modeling matrix.\n",
    "- Splits the dataset into features (`X`) and target (`y`), storing them in shared state.\n",
    "\n",
    "➡️ **Run after loading data. Re-run if you adjust the configuration.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aadeb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = PIPELINE_STATE.get(\"df_raw\")\n",
    "if df_raw is None:\n",
    "    raise RuntimeError(\"Load data before running the target detection step.\")\n",
    "\n",
    "config = PIPELINE_STATE[\"config\"]\n",
    "user_target = config.get(\"target_column\")\n",
    "\n",
    "if not user_target or user_target not in df_raw.columns:\n",
    "    suggested = suggest_target_column(df_raw)\n",
    "    if suggested:\n",
    "        config[\"target_column\"] = suggested\n",
    "        print(f\"Target column auto-detected: {suggested}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unable to infer target column. Set config['target_column'] and re-run.\")\n",
    "else:\n",
    "    print(f\"Using user-specified target column: {user_target}\")\n",
    "\n",
    "PIPELINE_STATE[\"config\"] = config\n",
    "\n",
    "target_col = config[\"target_column\"]\n",
    "X = df_raw.drop(columns=[target_col])\n",
    "y = df_raw[target_col]\n",
    "\n",
    "X_clean, dropped_columns = drop_redundant_columns(X)\n",
    "if dropped_columns[\"constant\"] or dropped_columns[\"duplicates\"]:\n",
    "    print(\"Dropped redundant columns:\")\n",
    "    for kind, cols in dropped_columns.items():\n",
    "        if cols:\n",
    "            print(f\"  {kind}: {cols}\")\n",
    "else:\n",
    "    print(\"No redundant columns removed.\")\n",
    "\n",
    "PIPELINE_STATE[\"X\"] = X_clean\n",
    "PIPELINE_STATE[\"y\"] = y\n",
    "\n",
    "resolved_task = resolve_task_type(y, config.get(\"task_type\", \"auto\"))\n",
    "PIPELINE_STATE[\"task_type\"] = resolved_task\n",
    "print(f\"Resolved task type: {resolved_task}\")\n",
    "print(f\"Feature matrix shape: {X_clean.shape}; target length: {len(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e21f3c",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Exploratory Data Analysis (EDA) <a id=\"eda\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Generates summary statistics for numeric and categorical columns.\n",
    "- Visualises missingness, distributions, correlations, and target relationships.\n",
    "- Highlights keyboard-friendly guidance so you know which cell to run next.\n",
    "\n",
    "➡️ **Run to understand the dataset before modeling.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = PIPELINE_STATE.get(\"X\")\n",
    "y = PIPELINE_STATE.get(\"y\")\n",
    "resolved_task = PIPELINE_STATE.get(\"task_type\")\n",
    "if X is None or y is None:\n",
    "    raise RuntimeError(\"Ensure the target detection step has been executed.\")\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [col for col in X.columns if col not in numeric_cols]\n",
    "\n",
    "print(\"Numeric summary statistics:\")\n",
    "if numeric_cols:\n",
    "    display(describe_numeric(X[numeric_cols]))\n",
    "else:\n",
    "    print(\"No numeric columns.\")\n",
    "\n",
    "print(\"Categorical frequency snapshots:\")\n",
    "if cat_cols:\n",
    "    for col, summary in describe_categorical(X[cat_cols]).items():\n",
    "        display(summary.to_frame(name=\"count\"))\n",
    "else:\n",
    "    print(\"No categorical columns.\")\n",
    "\n",
    "print(\"Missingness overview:\")\n",
    "plot_missingness(pd.concat([X, y.rename(\"__target__\")], axis=1))\n",
    "\n",
    "print(\"Numeric distributions:\")\n",
    "plot_numeric_distributions(X)\n",
    "\n",
    "print(\"Correlation heatmap:\")\n",
    "plot_correlation_heatmap(pd.concat([X[numeric_cols], y], axis=1))\n",
    "\n",
    "print(\"Feature-target relationships:\")\n",
    "plot_target_relationships(X, y, resolved_task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6937ef3",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Outlier Detection & Removal <a id=\"outliers\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Flags potential outliers using configurable methods (IQR, Z-Score, IsolationForest).\n",
    "- Reports how many rows would be removed before applying the filter.\n",
    "- Lets you opt-in to removal explicitly so you stay in control of data hygiene.\n",
    "\n",
    "➡️ **Adjust the toggles below, then re-run if you change your mind.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af600978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = PIPELINE_STATE.get(\"X\")\n",
    "y = PIPELINE_STATE.get(\"y\")\n",
    "config = PIPELINE_STATE.get(\"config\", {})\n",
    "if X is None or y is None:\n",
    "    raise RuntimeError(\"Load data and identify the target before running outlier detection.\")\n",
    "\n",
    "OUTLIER_METHOD = config.get(\"outlier_method\")  # Override here if desired\n",
    "OUTLIER_THRESHOLD = config.get(\"outlier_threshold\", 1.5)\n",
    "APPLY_OUTLIER_FILTER = False  # Flip to True to remove flagged rows\n",
    "\n",
    "if OUTLIER_METHOD:\n",
    "    mask = compute_outlier_mask(pd.concat([X, y], axis=1), method=OUTLIER_METHOD, threshold=OUTLIER_THRESHOLD)\n",
    "    num_outliers = int(mask.sum())\n",
    "    print(f\"Outlier method: {OUTLIER_METHOD} (threshold={OUTLIER_THRESHOLD}). Rows flagged: {num_outliers} / {len(mask)}\")\n",
    "    if num_outliers > 0:\n",
    "        preview = pd.concat([X.loc[mask].head(5), y.loc[mask].head(5)], axis=1)\n",
    "        print(\"Preview of rows flagged as outliers:\")\n",
    "        display(preview)\n",
    "    if APPLY_OUTLIER_FILTER and num_outliers > 0:\n",
    "        X_filtered = X.loc[~mask].copy()\n",
    "        y_filtered = y.loc[~mask].copy()\n",
    "        PIPELINE_STATE[\"X\"] = X_filtered\n",
    "        PIPELINE_STATE[\"y\"] = y_filtered\n",
    "        PIPELINE_STATE.setdefault(\"run_notes\", []).append(f\"Removed {num_outliers} rows via {OUTLIER_METHOD} outlier filter.\")\n",
    "        print(f\"Applied outlier removal. New shape: {X_filtered.shape}\")\n",
    "    else:\n",
    "        print(\"Outliers not removed (APPLY_OUTLIER_FILTER=False). Re-run with True to drop them.\")\n",
    "else:\n",
    "    print(\"Outlier detection skipped. Set config['outlier_method'] to enable (IQR, ZScore, IsolationForest).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47203747",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Preprocessing Pipeline <a id=\"preprocessing\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Configures imputers, encoders, and scalers using scikit-learn pipelines.\n",
    "- Shows the final `ColumnTransformer` so you know exactly how features are processed.\n",
    "- Stores the preprocessing object for reuse during training and inference.\n",
    "\n",
    "➡️ **Run once you are satisfied with outlier handling.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98154042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = PIPELINE_STATE.get(\"X\")\n",
    "y = PIPELINE_STATE.get(\"y\")\n",
    "config = PIPELINE_STATE.get(\"config\", {})\n",
    "if X is None or y is None:\n",
    "    raise RuntimeError(\"Run previous steps to set X and y.\")\n",
    "\n",
    "preprocessor = build_preprocessing_pipeline(\n",
    "    X,\n",
    "    scaling=config.get(\"scaling\"),\n",
    "    encoding=config.get(\"categorical_encoding\"),\n",
    "    impute_strategy_num=config.get(\"impute_numeric\", \"median\"),\n",
    "    impute_strategy_cat=config.get(\"impute_categorical\", \"most_frequent\"),\n",
    ")\n",
    "PIPELINE_STATE[\"preprocessor\"] = preprocessor\n",
    "\n",
    "print(\"Preprocessing pipeline configured:\")\n",
    "print(preprocessor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a868eaf",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Data Splitting Strategy <a id=\"splitting\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Performs train/validation/test splits with stratification when appropriate.\n",
    "- Summarises target distribution across splits (classification) or value range (regression).\n",
    "- Prepares optional cross-validation folds if requested.\n",
    "\n",
    "➡️ **Run after preprocessing is set.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = PIPELINE_STATE.get(\"X\")\n",
    "y = PIPELINE_STATE.get(\"y\")\n",
    "preprocessor = PIPELINE_STATE.get(\"preprocessor\")\n",
    "config = PIPELINE_STATE.get(\"config\", {})\n",
    "resolved_task = PIPELINE_STATE.get(\"task_type\", \"classification\")\n",
    "if X is None or y is None or preprocessor is None:\n",
    "    raise RuntimeError(\"Ensure X, y, and the preprocessing pipeline are ready before splitting.\")\n",
    "\n",
    "test_size = config.get(\"test_size\", 0.2)\n",
    "val_size = config.get(\"val_size\", 0.1)\n",
    "stratify_arg = y if resolved_task == \"classification\" else None\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=test_size,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=stratify_arg,\n",
    ")\n",
    "\n",
    "if val_size and val_size > 0:\n",
    "    adjusted_val_size = val_size / (1 - test_size)\n",
    "    stratify_temp = y_temp if resolved_task == \"classification\" else None\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=adjusted_val_size,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=stratify_temp,\n",
    "    )\n",
    "else:\n",
    "    X_train, y_train = X_temp, y_temp\n",
    "    X_val = pd.DataFrame()\n",
    "    y_val = pd.Series(dtype=y.dtype)\n",
    "\n",
    "PIPELINE_STATE.update(\n",
    "    {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "if not X_val.empty:\n",
    "    print(f\"Validation set: {X_val.shape}\")\n",
    "else:\n",
    "    print(\"Validation set: not created (val_size=0 or not provided).\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "if resolved_task == \"classification\":\n",
    "    for split_name, target_split in {\"train\": y_train, \"test\": y_test, \"val\": y_val}.items():\n",
    "        if target_split.empty:\n",
    "            continue\n",
    "        counts = target_split.value_counts(normalize=True)\n",
    "        display(pd.DataFrame({\"proportion\": counts, \"count\": target_split.value_counts()}).rename_axis(\"class\"))\n",
    "else:\n",
    "    for split_name, target_split in {\"train\": y_train, \"test\": y_test, \"val\": y_val}.items():\n",
    "        if target_split.empty:\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.hist(target_split, bins=20, color=\"#bf5b17\", alpha=0.75)\n",
    "        plt.title(f\"Target distribution – {split_name}\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "if config.get(\"cv_folds\", 0) and config[\"cv_folds\"] > 1:\n",
    "    print(f\"Cross-validation enabled: {config['cv_folds']} folds.\")\n",
    "else:\n",
    "    print(\"Cross-validation disabled (cv_folds <= 1).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35edd973",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Model Zoo Setup <a id=\"model-zoo\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Builds a catalogue of baseline and advanced estimators based on task type and library availability.\n",
    "- Keeps model configurations modest for quick experimentation on CPU.\n",
    "- Stores ready-to-train pipelines that combine preprocessing with each estimator.\n",
    "\n",
    "➡️ **Run before the training stage.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, LinearRegression\n",
    "\n",
    "config = PIPELINE_STATE.get(\"config\", {})\n",
    "resolved_task = PIPELINE_STATE.get(\"task_type\", \"classification\")\n",
    "preprocessor = PIPELINE_STATE.get(\"preprocessor\")\n",
    "if preprocessor is None:\n",
    "    raise RuntimeError(\"Preprocessor not configured. Run the preprocessing cell first.\")\n",
    "\n",
    "model_specs: List[Tuple[str, Any]] = []\n",
    "\n",
    "if resolved_task == \"classification\":\n",
    "    model_specs.extend(\n",
    "        [\n",
    "            (\"LogisticRegression\", LogisticRegression(max_iter=1000, class_weight=\"balanced\")),\n",
    "            (\"RandomForestClassifier\", RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED)),\n",
    "        ]\n",
    "    )\n",
    "    if config.get(\"enable_xgboost\") and HAS_XGBOOST:\n",
    "        model_specs.append(\n",
    "            (\n",
    "                \"XGBoostClassifier\",\n",
    "                OPTIONAL_MODULES[\"xgboost\"].XGBClassifier(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=4,\n",
    "                    learning_rate=0.1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    eval_metric=\"logloss\",\n",
    "                    random_state=RANDOM_SEED,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    if config.get(\"enable_lightgbm\") and HAS_LIGHTGBM:\n",
    "        model_specs.append(\n",
    "            (\n",
    "                \"LightGBMClassifier\",\n",
    "                OPTIONAL_MODULES[\"lightgbm\"].LGBMClassifier(\n",
    "                    n_estimators=200,\n",
    "                    learning_rate=0.1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=RANDOM_SEED,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    model_specs.extend(\n",
    "        [\n",
    "            (\"LinearRegression\", LinearRegression()),\n",
    "            (\"Ridge\", Ridge(alpha=1.0, random_state=RANDOM_SEED)),\n",
    "            (\"RandomForestRegressor\", RandomForestRegressor(n_estimators=250, random_state=RANDOM_SEED)),\n",
    "        ]\n",
    "    )\n",
    "    if config.get(\"enable_xgboost\") and HAS_XGBOOST:\n",
    "        model_specs.append(\n",
    "            (\n",
    "                \"XGBoostRegressor\",\n",
    "                OPTIONAL_MODULES[\"xgboost\"].XGBRegressor(\n",
    "                    n_estimators=300,\n",
    "                    max_depth=4,\n",
    "                    learning_rate=0.1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=RANDOM_SEED,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    if config.get(\"enable_lightgbm\") and HAS_LIGHTGBM:\n",
    "        model_specs.append(\n",
    "            (\n",
    "                \"LightGBMRegressor\",\n",
    "                OPTIONAL_MODULES[\"lightgbm\"].LGBMRegressor(\n",
    "                    n_estimators=300,\n",
    "                    learning_rate=0.1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=RANDOM_SEED,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "models: Dict[str, Pipeline] = {}\n",
    "for name, estimator in model_specs:\n",
    "    models[name] = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", estimator)])\n",
    "\n",
    "PIPELINE_STATE[\"models\"] = models\n",
    "print(\"Models configured:\")\n",
    "for name in models:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "if resolved_task == \"classification\" and config.get(\"enable_pytorch_nn\") and HAS_TORCH:\n",
    "    print(\"PyTorch detected; neural network head will be initialised during training if requested.\")\n",
    "elif config.get(\"enable_pytorch_nn\"):\n",
    "    print(\"PyTorch requested but not available; skipping neural network setup.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4672e6",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Training & Live Progress <a id=\"training\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Fits each configured model with timing information and optional cross-validation.\n",
    "- Tracks validation/test metrics for leaderboard comparison.\n",
    "- Provides a PyTorch MLP training loop with live loss curves when the library is available.\n",
    "\n",
    "➡️ **Run to train all models. Adjust epochs or selection as needed.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af644ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "models = PIPELINE_STATE.get(\"models\", {})\n",
    "config = PIPELINE_STATE.get(\"config\", {})\n",
    "resolved_task = PIPELINE_STATE.get(\"task_type\", \"classification\")\n",
    "X_train = PIPELINE_STATE.get(\"X_train\")\n",
    "y_train = PIPELINE_STATE.get(\"y_train\")\n",
    "X_val = PIPELINE_STATE.get(\"X_val\")\n",
    "y_val = PIPELINE_STATE.get(\"y_val\")\n",
    "X_test = PIPELINE_STATE.get(\"X_test\")\n",
    "y_test = PIPELINE_STATE.get(\"y_test\")\n",
    "preprocessor = PIPELINE_STATE.get(\"preprocessor\")\n",
    "\n",
    "if not models or X_train is None or y_train is None:\n",
    "    raise RuntimeError(\"Ensure previous steps configured models and data splits.\")\n",
    "\n",
    "results: List[ModelResult] = []\n",
    "leaderboard_rows: List[Dict[str, Any]] = []\n",
    "cv_folds = config.get(\"cv_folds\", 0)\n",
    "\n",
    "PRIMARY_METRIC = \"F1 (macro)\" if resolved_task == \"classification\" else \"R2\"\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    start = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    metrics: Dict[str, float] = {\"train_time_s\": float(train_time)}\n",
    "    val_metric = None\n",
    "    test_metric = None\n",
    "\n",
    "    if cv_folds and cv_folds > 1:\n",
    "        scoring = \"f1_macro\" if resolved_task == \"classification\" else \"r2\"\n",
    "        cv_cls = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_SEED) if resolved_task == \"classification\" else KFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv_cls, scoring=scoring)\n",
    "        metrics[\"cv_mean\"] = float(np.mean(cv_scores))\n",
    "        metrics[\"cv_std\"] = float(np.std(cv_scores))\n",
    "\n",
    "    if not X_val.empty and not y_val.empty:\n",
    "        val_pred = pipeline.predict(X_val)\n",
    "        if resolved_task == \"classification\":\n",
    "            val_metric = f1_score(y_val, val_pred, average=\"macro\")\n",
    "            metrics[\"val_f1_macro\"] = float(val_metric)\n",
    "            metrics[\"val_accuracy\"] = float(accuracy_score(y_val, val_pred))\n",
    "        else:\n",
    "            val_metric = r2_score(y_val, val_pred)\n",
    "            metrics[\"val_r2\"] = float(val_metric)\n",
    "            metrics[\"val_rmse\"] = float(mean_squared_error(y_val, val_pred, squared=False))\n",
    "\n",
    "    test_pred = pipeline.predict(X_test)\n",
    "    if resolved_task == \"classification\":\n",
    "        test_metric = f1_score(y_test, test_pred, average=\"macro\")\n",
    "        metrics[\"test_f1_macro\"] = float(test_metric)\n",
    "        metrics[\"test_accuracy\"] = float(accuracy_score(y_test, test_pred))\n",
    "        if hasattr(pipeline.named_steps[\"model\"], \"predict_proba\"):\n",
    "            try:\n",
    "                proba = pipeline.predict_proba(X_test)\n",
    "                metrics[\"test_log_loss\"] = float(log_loss(y_test, proba))\n",
    "            except ValueError:\n",
    "                pass\n",
    "    else:\n",
    "        test_metric = r2_score(y_test, test_pred)\n",
    "        metrics[\"test_r2\"] = float(test_metric)\n",
    "        metrics[\"test_rmse\"] = float(mean_squared_error(y_test, test_pred, squared=False))\n",
    "        metrics[\"test_mae\"] = float(mean_absolute_error(y_test, test_pred))\n",
    "\n",
    "    results.append(\n",
    "        ModelResult(\n",
    "            name=name,\n",
    "            pipeline=pipeline,\n",
    "            train_time=train_time,\n",
    "            val_metric=val_metric,\n",
    "            test_metric=test_metric,\n",
    "            metrics=metrics,\n",
    "            task_type=resolved_task,\n",
    "            additional={\"predictions\": test_pred},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    leaderboard_rows.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Primary metric\": float(test_metric) if test_metric is not None else (float(val_metric) if val_metric is not None else np.nan),\n",
    "            \"Train time\": format_seconds(train_time),\n",
    "        }\n",
    "    )\n",
    "    print(f\"Trained {name} in {format_seconds(train_time)}\")\n",
    "\n",
    "# Optional PyTorch neural network\n",
    "if config.get(\"enable_pytorch_nn\") and HAS_TORCH and preprocessor is not None:\n",
    "    torch = OPTIONAL_MODULES[\"torch\"]\n",
    "    nn = torch.nn\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    preprocessor_nn = clone(preprocessor)\n",
    "    X_train_trans = preprocessor_nn.fit_transform(X_train)\n",
    "    X_test_trans = preprocessor_nn.transform(X_test)\n",
    "    if not X_val.empty:\n",
    "        X_val_trans = preprocessor_nn.transform(X_val)\n",
    "    else:\n",
    "        X_val_trans = None\n",
    "\n",
    "    if resolved_task == \"classification\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_enc = torch.tensor(label_encoder.fit_transform(y_train), dtype=torch.long)\n",
    "        y_test_enc = torch.tensor(label_encoder.transform(y_test), dtype=torch.long)\n",
    "        if X_val_trans is not None:\n",
    "            y_val_enc = torch.tensor(label_encoder.transform(y_val), dtype=torch.long)\n",
    "        output_dim = len(label_encoder.classes_)\n",
    "    else:\n",
    "        label_encoder = None\n",
    "        y_train_enc = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "        y_test_enc = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "        if X_val_trans is not None:\n",
    "            y_val_enc = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "        output_dim = 1\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train_trans, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_trans, dtype=torch.float32)\n",
    "    if X_val_trans is not None:\n",
    "        X_val_tensor = torch.tensor(X_val_trans, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_enc)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    class SimpleTabularNN(nn.Module):\n",
    "        def __init__(self, input_dim: int, output_dim: int, hidden_layers: List[int], dropout: float = 0.1):\n",
    "            super().__init__()\n",
    "            layers: List[nn.Module] = []\n",
    "            prev_dim = input_dim\n",
    "            for hidden_dim in hidden_layers:\n",
    "                layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                prev_dim = hidden_dim\n",
    "            layers.append(nn.Linear(prev_dim, output_dim))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            return self.network(x)\n",
    "\n",
    "    input_dim = X_train_trans.shape[1]\n",
    "    hidden_layers = [128, 64]\n",
    "    model = SimpleTabularNN(input_dim, output_dim, hidden_layers, dropout=0.2).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss() if resolved_task == \"classification\" else torch.nn.MSELoss()\n",
    "\n",
    "    max_epochs = 25\n",
    "    patience = 5\n",
    "    best_score = -np.inf\n",
    "    wait = 0\n",
    "    history_train_loss: List[float] = []\n",
    "    history_metric: List[float] = []\n",
    "    torch_start = time.time()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            if resolved_task == \"classification\":\n",
    "                loss = loss_fn(outputs, batch_y)\n",
    "            else:\n",
    "                loss = loss_fn(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * batch_x.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        history_train_loss.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if X_val_trans is not None:\n",
    "                val_outputs = model(X_val_tensor.to(device))\n",
    "                if resolved_task == \"classification\":\n",
    "                    val_probs = torch.softmax(val_outputs, dim=1)\n",
    "                    val_preds = val_probs.argmax(dim=1).cpu().numpy()\n",
    "                    val_metric = f1_score(y_val, label_encoder.inverse_transform(val_preds), average=\"macro\")\n",
    "                else:\n",
    "                    val_preds = val_outputs.squeeze().cpu().numpy()\n",
    "                    val_metric = r2_score(y_val, val_preds)\n",
    "            else:\n",
    "                train_outputs = model(X_train_tensor.to(device))\n",
    "                if resolved_task == \"classification\":\n",
    "                    train_probs = torch.softmax(train_outputs, dim=1)\n",
    "                    train_preds = train_probs.argmax(dim=1).cpu().numpy()\n",
    "                    val_metric = f1_score(y_train, label_encoder.inverse_transform(train_preds), average=\"macro\")\n",
    "                else:\n",
    "                    train_preds = train_outputs.squeeze().cpu().numpy()\n",
    "                    val_metric = r2_score(y_train, train_preds)\n",
    "        history_metric.append(val_metric)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(history_train_loss, label=\"Train loss\", color=\"#1b9e77\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"PyTorch MLP training progress\")\n",
    "        ax2 = plt.gca().twinx()\n",
    "        ax2.plot(history_metric, label=\"Validation metric\", color=\"#d95f02\")\n",
    "        ax2.set_ylabel(\"Metric\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs} | loss={epoch_loss:.4f} | metric={val_metric:.4f}\")\n",
    "\n",
    "        if val_metric > best_score:\n",
    "            best_score = val_metric\n",
    "            wait = 0\n",
    "            best_state = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"preprocessor\": preprocessor_nn,\n",
    "            }\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    if 'best_state' in locals():\n",
    "        model.load_state_dict(best_state[\"model\"])\n",
    "        preprocessor_nn = best_state[\"preprocessor\"]\n",
    "\n",
    "    torch_train_time = time.time() - torch_start\n",
    "\n",
    "    class TorchPipeline:\n",
    "        def __init__(self, preprocessor, model, task_type, label_encoder, device):\n",
    "            self.preprocessor = preprocessor\n",
    "            self.model = model\n",
    "            self.task_type = task_type\n",
    "            self.label_encoder = label_encoder\n",
    "            self.device = device\n",
    "            self.model.eval()\n",
    "\n",
    "        def predict(self, X_df: pd.DataFrame) -> np.ndarray:\n",
    "            transformed = self.preprocessor.transform(X_df)\n",
    "            tensor = torch.tensor(transformed, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(tensor)\n",
    "            if self.task_type == \"classification\":\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                return self.label_encoder.inverse_transform(preds)\n",
    "            return outputs.squeeze().cpu().numpy()\n",
    "\n",
    "        def predict_proba(self, X_df: pd.DataFrame) -> np.ndarray:\n",
    "            if self.task_type != \"classification\":\n",
    "                raise AttributeError(\"predict_proba is classification-only.\")\n",
    "            transformed = self.preprocessor.transform(X_df)\n",
    "            tensor = torch.tensor(transformed, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(tensor)\n",
    "                probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            return probs\n",
    "\n",
    "    torch_pipeline = TorchPipeline(preprocessor_nn, model, resolved_task, label_encoder, device)\n",
    "\n",
    "    torch_test_pred = torch_pipeline.predict(X_test)\n",
    "    torch_metrics: Dict[str, float] = {\"train_time_s\": float(torch_train_time)}\n",
    "    if resolved_task == \"classification\":\n",
    "        torch_test_metric = f1_score(y_test, torch_test_pred, average=\"macro\")\n",
    "        torch_metrics[\"test_f1_macro\"] = float(torch_test_metric)\n",
    "        torch_metrics[\"test_accuracy\"] = float(accuracy_score(y_test, torch_test_pred))\n",
    "    else:\n",
    "        torch_test_metric = r2_score(y_test, torch_test_pred)\n",
    "        torch_metrics[\"test_r2\"] = float(torch_test_metric)\n",
    "        torch_metrics[\"test_rmse\"] = float(mean_squared_error(y_test, torch_test_pred, squared=False))\n",
    "        torch_metrics[\"test_mae\"] = float(mean_absolute_error(y_test, torch_test_pred))\n",
    "\n",
    "    results.append(\n",
    "        ModelResult(\n",
    "            name=\"PyTorchMLP\",\n",
    "            pipeline=torch_pipeline,\n",
    "            train_time=torch_train_time,\n",
    "            val_metric=best_score if 'best_state' in locals() else None,\n",
    "            test_metric=torch_test_metric,\n",
    "            metrics=torch_metrics,\n",
    "            task_type=resolved_task,\n",
    "            additional={\"history_loss\": history_train_loss, \"history_metric\": history_metric},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    leaderboard_rows.append(\n",
    "        {\n",
    "            \"Model\": \"PyTorchMLP\",\n",
    "            \"Primary metric\": float(torch_test_metric),\n",
    "            \"Train time\": format_seconds(torch_train_time),\n",
    "        }\n",
    "    )\n",
    "    print(\"PyTorch MLP training complete.\")\n",
    "elif config.get(\"enable_pytorch_nn\") and not HAS_TORCH:\n",
    "    print(\"PyTorch was requested but is not installed; skipping neural network training.\")\n",
    "\n",
    "PIPELINE_STATE[\"model_results\"] = results\n",
    "PIPELINE_STATE[\"leaderboard_rows\"] = leaderboard_rows\n",
    "\n",
    "print(\"\n",
    "Training complete. Proceed to the Evaluation section for detailed metrics.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636c319",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Evaluation <a id=\"evaluation\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Computes rich metrics (classification or regression) on the held-out test set.\n",
    "- Visualises confusion matrices, ROC/PR curves, or regression diagnostics.\n",
    "- Produces a leaderboard sorted by the primary metric for quick comparison.\n",
    "\n",
    "➡️ **Run after training to analyse performance.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results: List[ModelResult] = PIPELINE_STATE.get(\"model_results\", [])\n",
    "leaderboard_rows = PIPELINE_STATE.get(\"leaderboard_rows\", [])\n",
    "resolved_task = PIPELINE_STATE.get(\"task_type\", \"classification\")\n",
    "X_test = PIPELINE_STATE.get(\"X_test\")\n",
    "y_test = PIPELINE_STATE.get(\"y_test\")\n",
    "\n",
    "if not results:\n",
    "    raise RuntimeError(\"No trained models found. Run the training cell first.\")\n",
    "\n",
    "leaderboard = pd.DataFrame(leaderboard_rows)\n",
    "leaderboard = leaderboard.sort_values(by=\"Primary metric\", ascending=False)\n",
    "print(\"Model leaderboard (sorted by primary metric):\")\n",
    "display(leaderboard.reset_index(drop=True))\n",
    "\n",
    "for result in results:\n",
    "    pipeline = result.pipeline\n",
    "    name = result.name\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Train time: {format_seconds(result.train_time)}\")\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    if resolved_task == \"classification\":\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.imshow(cm, cmap=\"Blues\")\n",
    "        plt.title(f\"Confusion matrix – {name}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "        if hasattr(pipeline, \"predict_proba\"):\n",
    "            try:\n",
    "                proba = pipeline.predict_proba(X_test)\n",
    "            except Exception:\n",
    "                proba = None\n",
    "        else:\n",
    "            proba = None\n",
    "\n",
    "        if proba is not None:\n",
    "            classes = np.unique(y_test)\n",
    "            if len(classes) == 2:\n",
    "                fpr, tpr, _ = roc_curve(y_test, proba[:, 1], pos_label=classes[1])\n",
    "                plt.figure(figsize=(5, 4))\n",
    "                plt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc_score(y_test, proba[:, 1]):.3f}\")\n",
    "                plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(f\"ROC Curve – {name}\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                precision, recall, _ = precision_recall_curve(y_test, proba[:, 1], pos_label=classes[1])\n",
    "                plt.figure(figsize=(5, 4))\n",
    "                plt.plot(recall, precision, color=\"#7fc97f\")\n",
    "                plt.xlabel(\"Recall\")\n",
    "                plt.ylabel(\"Precision\")\n",
    "                plt.title(f\"Precision-Recall Curve – {name}\")\n",
    "                plt.show()\n",
    "            else:\n",
    "                try:\n",
    "                    roc_auc = roc_auc_score(y_test, proba, multi_class=\"ovr\")\n",
    "                    print(f\"Macro ROC AUC: {roc_auc:.3f}\")\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        print(\"Interpretation: Higher recall/precision and balanced confusion matrices indicate robust classifiers.\")\n",
    "    else:\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"MAE: {mae:.3f} | RMSE: {rmse:.3f} | R2: {r2:.3f}\")\n",
    "        residuals = y_test - y_pred\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.scatter(y_pred, residuals, alpha=0.6, color=\"#386cb0\")\n",
    "        plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "        plt.title(f\"Residuals vs Prediction – {name}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.6, color=\"#f0027f\")\n",
    "        min_val = min(y_test.min(), y_pred.min())\n",
    "        max_val = max(y_test.max(), y_pred.max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", color=\"gray\")\n",
    "        plt.title(f\"Predicted vs True – {name}\")\n",
    "        plt.xlabel(\"True\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.show()\n",
    "        print(\"Interpretation: Points hugging the diagonal indicate well-calibrated regression predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290412f1",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Inference Demo <a id=\"inference\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Shows how to generate predictions on the held-out test set.\n",
    "- Demonstrates inline dictionary-to-DataFrame conversion for single-sample inference.\n",
    "- Keeps everything in-memory for quick experimentation.\n",
    "\n",
    "➡️ **Run to validate prediction APIs for the trained models.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results: List[ModelResult] = PIPELINE_STATE.get(\"model_results\", [])\n",
    "X_test = PIPELINE_STATE.get(\"X_test\")\n",
    "y_test = PIPELINE_STATE.get(\"y_test\")\n",
    "\n",
    "if not results:\n",
    "    raise RuntimeError(\"Train models before running inference.\")\n",
    "\n",
    "sample_rows = X_test.head(3)\n",
    "print(\"Test set sample predictions:\")\n",
    "for result in results:\n",
    "    preds = result.pipeline.predict(sample_rows)\n",
    "    print(f\"{result.name}: {preds}\")\n",
    "    if result.task_type == \"classification\" and hasattr(result.pipeline, \"predict_proba\"):\n",
    "        try:\n",
    "            probas = result.pipeline.predict_proba(sample_rows)\n",
    "            print(\"  Probabilities:\")\n",
    "            print(probas)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "custom_input = {col: sample_rows.iloc[0][col] for col in sample_rows.columns}\n",
    "print(\"\n",
    "Custom single-row inference (edit `custom_input` as needed):\")\n",
    "custom_df = pd.DataFrame([custom_input])\n",
    "for result in results:\n",
    "    pred = result.pipeline.predict(custom_df)[0]\n",
    "    print(f\"{result.name}: {pred}\")\n",
    "    if result.task_type == \"classification\" and hasattr(result.pipeline, \"predict_proba\"):\n",
    "        try:\n",
    "            print(\"  Probability vector:\", result.pipeline.predict_proba(custom_df)[0])\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a084b8",
   "metadata": {},
   "source": [
    "\n",
    "## 14. Run Summary <a id=\"run-summary\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Consolidates key run details: dataset source, rows used, preprocessing choices, and top model.\n",
    "- Provides a lightweight log for reproducibility (no disk writes).\n",
    "- Helps you verify the pipeline’s overall status at a glance.\n",
    "\n",
    "➡️ **Run after evaluation to summarise the workflow.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde23ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = PIPELINE_STATE.get(\"config\", {})\n",
    "results: List[ModelResult] = PIPELINE_STATE.get(\"model_results\", [])\n",
    "leaderboard = PIPELINE_STATE.get(\"leaderboard_rows\", [])\n",
    "X = PIPELINE_STATE.get(\"X\")\n",
    "y = PIPELINE_STATE.get(\"y\")\n",
    "run_notes = PIPELINE_STATE.get(\"run_notes\", [])\n",
    "\n",
    "if not results:\n",
    "    raise RuntimeError(\"Run training and evaluation before summarising.\")\n",
    "\n",
    "best_entry = max(leaderboard, key=lambda row: row.get(\"Primary metric\", float(\"-inf\")))\n",
    "best_model_name = best_entry[\"Model\"]\n",
    "best_metric_value = best_entry.get(\"Primary metric\")\n",
    "if isinstance(best_metric_value, (float, int)):\n",
    "    best_metric_display = f\"{best_metric_value:.4f}\"\n",
    "else:\n",
    "    best_metric_display = str(best_metric_value)\n",
    "\n",
    "summary_lines = [\n",
    "    f\"**Data source:** {PIPELINE_STATE.get('data_source', 'Unknown')}\",\n",
    "    f\"**Rows / columns:** {X.shape[0]} / {X.shape[1]}\",\n",
    "    f\"**Target column:** {config.get('target_column')}\",\n",
    "    f\"**Task type:** {PIPELINE_STATE.get('task_type')}\",\n",
    "    f\"**Scaling:** {config.get('scaling')} | **Categorical encoding:** {config.get('categorical_encoding')}\",\n",
    "    f\"**Outlier method:** {config.get('outlier_method') or 'None'}\",\n",
    "    f\"**Models trained:** {', '.join([r.name for r in results])}\",\n",
    "    f\"**Best model:** {best_model_name} (score={best_metric_display})\",\n",
    "]\n",
    "if run_notes:\n",
    "    summary_lines.append(\"**Notes:** \" + \" | \".join(run_notes))\n",
    "\n",
    "display(Markdown(\"\n",
    "\".join(summary_lines)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a8e55",
   "metadata": {},
   "source": [
    "\n",
    "## 15. Optional Export Cells <a id=\"optional-export\"></a>\n",
    "\n",
    "**What this step does**\n",
    "- Provides opt-in utilities for serialising the best model and exporting figures.\n",
    "- Only executes when `allow_export=True` to avoid unintended file writes.\n",
    "- Demonstrates how to keep artefacts in-memory or persist them locally.\n",
    "\n",
    "➡️ **Enable exports in the config cell and run this block explicitly.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = PIPELINE_STATE.get(\"config\", {})\n",
    "allow_export = bool(config.get(\"allow_export\"))\n",
    "results: List[ModelResult] = PIPELINE_STATE.get(\"model_results\", [])\n",
    "leaderboard = PIPELINE_STATE.get(\"leaderboard_rows\", [])\n",
    "\n",
    "if not allow_export:\n",
    "    print(\"Exports are disabled. Set config['allow_export']=True and re-run if needed.\")\n",
    "else:\n",
    "    import io\n",
    "    import joblib\n",
    "\n",
    "    best_entry = max(leaderboard, key=lambda row: row.get(\"Primary metric\", float(\"-inf\")))\n",
    "    best_model_name = best_entry[\"Model\"]\n",
    "    best_result = next(r for r in results if r.name == best_model_name)\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    joblib.dump(best_result.pipeline, buffer)\n",
    "    buffer.seek(0)\n",
    "    print(f\"Serialized pipeline for {best_model_name} to in-memory bytes (size={len(buffer.getvalue())/1024:.1f} KB).\")\n",
    "\n",
    "    EXPORT_PATH = Path(\"best_model_pipeline.joblib\")\n",
    "    # Uncomment the next line to persist locally (opt-in, small file).\n",
    "    # EXPORT_PATH.write_bytes(buffer.getvalue())\n",
    "    print(\"To persist on disk, uncomment the write line in this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40d074",
   "metadata": {},
   "source": [
    "\n",
    "## 16. Testing Checklist <a id=\"testing-checklist\"></a>\n",
    "\n",
    "- [ ] Load built-in Titanic-like sample → complete full pipeline.\n",
    "- [ ] Flip task to regression by choosing a numeric target (e.g., `Fare`) and rerun.\n",
    "- [ ] Simulate missing values in a copy and verify preprocessing handles them.\n",
    "- [ ] Toggle outlier removal methods and confirm row count changes and plots update.\n",
    "- [ ] If torch available: run 15–20 epochs, verify live curves & early stopping.\n",
    "- [ ] Confirm no disk writes occur unless `ALLOW_EXPORT=True` is set and export cells are executed.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
