{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "56eb9872",
      "metadata": {},
      "source": "# Tabular ML End-to-End Demo (Object-Oriented Edition)\n\nThis notebook provides a compact walkthrough of the tabular machine learning\nworkflow using the :class:`TabulaML` utility class. Each stage writes\nartefacts to disk so the notebook can focus on orchestration and inspection.\n"
    },
    {
      "cell_type": "markdown",
      "id": "d4c3de78",
      "metadata": {},
      "source": "## 1. Environment setup\n\nImport the workflow helper and create an instance that controls where\noutputs such as previews, metrics, and trained models are stored.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7409dbd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from tabula_ml import TabulaML\n",
        "\n",
        "OUTPUT_DIR = Path(\"tmp/demo_run\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "pipeline = TabulaML(output_dir=OUTPUT_DIR)\n",
        "print(f\"Artefacts will be written to: {OUTPUT_DIR.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5eb15fa",
      "metadata": {},
      "source": "## 2. Configure the workflow\n\nUpdate the configuration dictionary to fine-tune preprocessing choices,\nmodel export behaviour, and the target column. The helper returns the merged\nconfiguration for quick inspection.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd878613",
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"target_column\": \"Survived\",\n",
        "    \"test_size\": 0.25,\n",
        "    \"scaling\": \"standard\",\n",
        "    \"categorical_encoding\": \"onehot\",\n",
        "    \"outlier_method\": None,\n",
        "    \"allow_export\": False,\n",
        "}\n",
        "\n",
        "pipeline.set_config(**CONFIG)\n",
        "CONFIG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "812bd4cd",
      "metadata": {},
      "source": "## 3. Load data and resolve target\n\nProvide a path to a CSV (or dataframe) and let the helper infer or confirm\nthe target column. The preview CSV is saved to ``OUTPUT_DIR`` for reference.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "488bd2e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = Path(\"data/sample_datasets/titanic_sample.csv\")\n",
        "raw_df = pipeline.load_data(DATA_PATH, name=\"Titanic sample\")\n",
        "X, y = pipeline.resolve_target()\n",
        "\n",
        "print(f\"Loaded dataset with shape {raw_df.shape} and target column '{pipeline.config['target_column']}'.\")\n",
        "print(\"Dropped redundant columns:\", pipeline.state[\"dropped_columns\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "990f7bcf",
      "metadata": {},
      "source": "## 4. Explore the dataset\n\nThe helper produces summary tables for numeric and categorical variables and\na missingness report. Artefact paths are returned for quick opening.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45762da",
      "metadata": {},
      "outputs": [],
      "source": [
        "eda_outputs = pipeline.perform_eda()\n",
        "eda_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e09f62f",
      "metadata": {},
      "source": "## 5. Optional: detect outliers\n\nConfigure the detection strategy via ``CONFIG``. When enabled, the helper\nreturns a boolean mask that can be used to inspect or filter rows.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a02c71",
      "metadata": {},
      "outputs": [],
      "source": [
        "outlier_mask = pipeline.detect_outliers()\n",
        "print(f\"Detected {outlier_mask.sum()} potential outliers out of {len(outlier_mask)} rows using method: {pipeline.config['outlier_method']}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25b54b8",
      "metadata": {},
      "source": "## 6. Split, train, and evaluate models\n\nThe training stage automatically selects estimators suited for the task\n(classification for Titanic). Metrics are written to disk and surfaced here.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e35f333e",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = pipeline.split_data()\n",
        "\n",
        "trained_models = pipeline.train_models()\n",
        "print(f\"Trained {[model.name for model in trained_models]}\")\n",
        "\n",
        "evaluated = pipeline.evaluate_models()\n",
        "{model.name: model.metrics for model in evaluated}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d2ef879",
      "metadata": {},
      "source": "## 7. Summarise the run\n\nCollate the high-level details of the workflow. The JSON file includes\npointers to artefacts for reproducibility.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a931a6cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = pipeline.summarise_run()\n",
        "summary"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}